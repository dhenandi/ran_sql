{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cac19235",
   "metadata": {},
   "source": [
    "# RAN Database Data Visualization and Analysis\n",
    "\n",
    "This notebook provides comprehensive visualization and analysis of the imported RAN performance data.\n",
    "\n",
    "## Overview\n",
    "- **Purpose**: Analyze imported RAN 2G/4G performance data\n",
    "- **Data Source**: SQLite database with imported CSV data\n",
    "- **Analysis Types**: Statistical, temporal, geographic, and performance analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a447c9",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for data analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6b1a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Date and time handling\n",
    "from datetime import datetime, timedelta\n",
    "import dateutil.parser\n",
    "\n",
    "# Setup plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Enable interactive plots in Jupyter\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path().absolute().parent.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(\"ðŸ“š Libraries imported successfully!\")\n",
    "print(f\"ðŸ  Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af67ef3",
   "metadata": {},
   "source": [
    "## 2. Database Connection Setup\n",
    "\n",
    "Establish connection to the SQLite database and create helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7333cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database configuration\n",
    "DATABASE_PATH = PROJECT_ROOT / \"data\" / \"databases\" / \"ran_performance.db\"\n",
    "\n",
    "class RANDatabaseAnalyzer:\n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "        \n",
    "    def get_connection(self):\n",
    "        \"\"\"Get database connection.\"\"\"\n",
    "        return sqlite3.connect(self.db_path)\n",
    "    \n",
    "    def execute_query(self, query, params=None):\n",
    "        \"\"\"Execute a query and return results as DataFrame.\"\"\"\n",
    "        with self.get_connection() as conn:\n",
    "            return pd.read_sql_query(query, conn, params=params)\n",
    "    \n",
    "    def get_table_list(self):\n",
    "        \"\"\"Get list of all tables in the database.\"\"\"\n",
    "        query = \"SELECT name FROM sqlite_master WHERE type='table'\"\n",
    "        return self.execute_query(query)['name'].tolist()\n",
    "    \n",
    "    def get_table_info(self, table_name):\n",
    "        \"\"\"Get detailed information about a table.\"\"\"\n",
    "        # Get column info\n",
    "        pragma_query = f\"PRAGMA table_info({table_name})\"\n",
    "        columns_df = self.execute_query(pragma_query)\n",
    "        \n",
    "        # Get row count\n",
    "        count_query = f\"SELECT COUNT(*) as row_count FROM {table_name}\"\n",
    "        row_count = self.execute_query(count_query)['row_count'].iloc[0]\n",
    "        \n",
    "        return {\n",
    "            'table_name': table_name,\n",
    "            'row_count': row_count,\n",
    "            'columns': columns_df\n",
    "        }\n",
    "    \n",
    "    def sample_data(self, table_name, n=1000):\n",
    "        \"\"\"Get a random sample of data from a table.\"\"\"\n",
    "        query = f\"SELECT * FROM {table_name} ORDER BY RANDOM() LIMIT {n}\"\n",
    "        return self.execute_query(query)\n",
    "\n",
    "# Initialize analyzer\n",
    "if DATABASE_PATH.exists():\n",
    "    analyzer = RANDatabaseAnalyzer(DATABASE_PATH)\n",
    "    print(f\"âœ… Connected to database: {DATABASE_PATH}\")\n",
    "else:\n",
    "    print(f\"âŒ Database not found: {DATABASE_PATH}\")\n",
    "    print(\"ðŸ’¡ Run the import script first to create the database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae724faa",
   "metadata": {},
   "source": [
    "## 3. Database Schema Overview\n",
    "\n",
    "Display all tables, their schemas, and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56319d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tables\n",
    "tables = analyzer.get_table_list()\n",
    "print(f\"ðŸ“Š Found {len(tables)} tables in the database:\")\n",
    "\n",
    "# Display table information\n",
    "table_summary = []\n",
    "for table in tables:\n",
    "    info = analyzer.get_table_info(table)\n",
    "    table_summary.append({\n",
    "        'Table': table,\n",
    "        'Rows': f\"{info['row_count']:,}\",\n",
    "        'Columns': len(info['columns']),\n",
    "        'Technology': '2G' if '2g' in table.lower() else '4G' if '4g' in table.lower() else 'Unknown'\n",
    "    })\n",
    "    print(f\"   ðŸ“‹ {table}: {info['row_count']:,} rows, {len(info['columns'])} columns\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(table_summary)\n",
    "print(\"\\nðŸ“ˆ Database Summary:\")\n",
    "display(summary_df)\n",
    "\n",
    "# Total rows across all tables\n",
    "total_rows = sum([int(row['Rows'].replace(',', '')) for row in table_summary])\n",
    "print(f\"\\nðŸŽ¯ Total rows across all tables: {total_rows:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498d1912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed schema for each table\n",
    "print(\"ðŸ” Detailed Schema Information:\")\n",
    "\n",
    "for table in tables[:2]:  # Show first 2 tables to avoid clutter\n",
    "    print(f\"\\nðŸ“‹ Table: {table}\")\n",
    "    info = analyzer.get_table_info(table)\n",
    "    columns_df = info['columns']\n",
    "    \n",
    "    # Display column information\n",
    "    display(columns_df[['name', 'type', 'notnull', 'dflt_value']].head(10))\n",
    "    \n",
    "    if len(columns_df) > 10:\n",
    "        print(f\"   ... and {len(columns_df) - 10} more columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79561280",
   "metadata": {},
   "source": [
    "## 4. Data Summary Statistics\n",
    "\n",
    "Generate descriptive statistics for numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78801e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample data from each table for analysis\n",
    "sample_data = {}\n",
    "for table in tables:\n",
    "    try:\n",
    "        sample_data[table] = analyzer.sample_data(table, n=5000)\n",
    "        print(f\"âœ… Loaded {len(sample_data[table])} sample rows from {table}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading {table}: {e}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Sample data loaded from {len(sample_data)} tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adacfdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics for each table\n",
    "for table_name, df in sample_data.items():\n",
    "    print(f\"\\nðŸ“ˆ Descriptive Statistics for {table_name}:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    \n",
    "    # Get numerical columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(numeric_cols) > 0:\n",
    "        stats_df = df[numeric_cols].describe()\n",
    "        display(stats_df.round(2))\n",
    "        \n",
    "        # Missing values\n",
    "        missing_data = df.isnull().sum()\n",
    "        missing_pct = (missing_data / len(df)) * 100\n",
    "        missing_summary = pd.DataFrame({\n",
    "            'Missing Count': missing_data,\n",
    "            'Missing %': missing_pct.round(2)\n",
    "        }).query('`Missing Count` > 0')\n",
    "        \n",
    "        if not missing_summary.empty:\n",
    "            print(\"\\nðŸš¨ Missing Data Summary:\")\n",
    "            display(missing_summary)\n",
    "    else:\n",
    "        print(\"No numerical columns found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f3d263",
   "metadata": {},
   "source": [
    "## 5. Technology Distribution Analysis\n",
    "\n",
    "Analyze the distribution of different RAN technologies in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25771a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technology distribution across tables\n",
    "tech_distribution = {}\n",
    "for table_name, df in sample_data.items():\n",
    "    if '2g' in table_name.lower():\n",
    "        tech_distribution['2G'] = tech_distribution.get('2G', 0) + len(df)\n",
    "    elif '4g' in table_name.lower():\n",
    "        tech_distribution['4G'] = tech_distribution.get('4G', 0) + len(df)\n",
    "    else:\n",
    "        tech_distribution['Other'] = tech_distribution.get('Other', 0) + len(df)\n",
    "\n",
    "# Create pie chart for technology distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Pie chart\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']\n",
    "wedges, texts, autotexts = ax1.pie(tech_distribution.values(), \n",
    "                                  labels=tech_distribution.keys(),\n",
    "                                  autopct='%1.1f%%',\n",
    "                                  colors=colors)\n",
    "ax1.set_title('ðŸ“¡ RAN Technology Distribution (Sample Data)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "bars = ax2.bar(tech_distribution.keys(), tech_distribution.values(), color=colors)\n",
    "ax2.set_title('ðŸ“Š Sample Data Count by Technology', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Number of Records')\n",
    "ax2.set_xlabel('Technology')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.annotate(f'{int(height):,}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“‹ Technology Distribution Summary:\")\n",
    "for tech, count in tech_distribution.items():\n",
    "    print(f\"   {tech}: {count:,} records ({count/sum(tech_distribution.values())*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0cd56d",
   "metadata": {},
   "source": [
    "## 6. Performance Metrics Visualization\n",
    "\n",
    "Analyze key performance indicators across different technologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bcf6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify common performance metrics columns\n",
    "performance_keywords = ['throughput', 'latency', 'signal', 'rssi', 'rsrp', 'rsrq', 'sinr', \n",
    "                       'bler', 'utilization', 'availability', 'quality', 'strength']\n",
    "\n",
    "perf_columns = {}\n",
    "for table_name, df in sample_data.items():\n",
    "    cols = [col for col in df.columns if any(keyword in col.lower() for keyword in performance_keywords)]\n",
    "    numeric_cols = df[cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if numeric_cols:\n",
    "        perf_columns[table_name] = numeric_cols\n",
    "\n",
    "print(\"ðŸŽ¯ Identified Performance Metrics:\")\n",
    "for table, cols in perf_columns.items():\n",
    "    print(f\"   {table}: {len(cols)} metrics - {cols[:3]}{'...' if len(cols) > 3 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6668a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance metrics visualization\n",
    "if perf_columns:\n",
    "    # Select first table with performance metrics\n",
    "    main_table = list(perf_columns.keys())[0]\n",
    "    main_df = sample_data[main_table]\n",
    "    main_cols = perf_columns[main_table][:6]  # First 6 metrics\n",
    "    \n",
    "    if main_cols:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, col in enumerate(main_cols):\n",
    "            if i < len(axes):\n",
    "                ax = axes[i]\n",
    "                \n",
    "                # Remove outliers for better visualization\n",
    "                data = main_df[col].dropna()\n",
    "                if len(data) > 0:\n",
    "                    Q1 = data.quantile(0.25)\n",
    "                    Q3 = data.quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    filtered_data = data[(data >= Q1 - 1.5 * IQR) & (data <= Q3 + 1.5 * IQR)]\n",
    "                    \n",
    "                    # Create histogram\n",
    "                    ax.hist(filtered_data, bins=30, alpha=0.7, color=colors[i % len(colors)], edgecolor='black')\n",
    "                    ax.set_title(f'{col}\\n(n={len(filtered_data):,})', fontweight='bold')\n",
    "                    ax.set_xlabel('Value')\n",
    "                    ax.set_ylabel('Frequency')\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # Add statistics text\n",
    "                    mean_val = filtered_data.mean()\n",
    "                    std_val = filtered_data.std()\n",
    "                    ax.axvline(mean_val, color='red', linestyle='--', alpha=0.8, label=f'Mean: {mean_val:.2f}')\n",
    "                    ax.legend()\n",
    "        \n",
    "        # Remove empty subplots\n",
    "        for i in range(len(main_cols), len(axes)):\n",
    "            fig.delaxes(axes[i])\n",
    "        \n",
    "        plt.suptitle(f'ðŸ“Š Performance Metrics Distribution - {main_table}', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸ No performance metrics columns identified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d872d42",
   "metadata": {},
   "source": [
    "## 7. Time Series Analysis\n",
    "\n",
    "Analyze temporal patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6da627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find timestamp columns\n",
    "timestamp_cols = ['last_update', 'timestamp', 'date', 'time']\n",
    "time_data = {}\n",
    "\n",
    "for table_name, df in sample_data.items():\n",
    "    for col in timestamp_cols:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                # Convert to datetime\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                time_data[table_name] = {'df': df, 'time_col': col}\n",
    "                print(f\"âœ… Found timestamp column '{col}' in {table_name}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error processing {col} in {table_name}: {e}\")\n",
    "\n",
    "print(f\"\\nâ° Found time data in {len(time_data)} tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d712b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series visualization\n",
    "if time_data:\n",
    "    fig, axes = plt.subplots(len(time_data), 1, figsize=(15, 6 * len(time_data)))\n",
    "    if len(time_data) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (table_name, data) in enumerate(time_data.items()):\n",
    "        df = data['df']\n",
    "        time_col = data['time_col']\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Create time series plot\n",
    "        df_clean = df.dropna(subset=[time_col])\n",
    "        \n",
    "        if len(df_clean) > 0:\n",
    "            # Group by hour and count records\n",
    "            df_clean['hour'] = df_clean[time_col].dt.hour\n",
    "            hourly_counts = df_clean.groupby('hour').size()\n",
    "            \n",
    "            ax.plot(hourly_counts.index, hourly_counts.values, marker='o', linewidth=2, markersize=6)\n",
    "            ax.set_title(f'ðŸ“… Hourly Data Distribution - {table_name}', fontweight='bold')\n",
    "            ax.set_xlabel('Hour of Day')\n",
    "            ax.set_ylabel('Number of Records')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_xticks(range(0, 24, 2))\n",
    "            \n",
    "            # Add date range info\n",
    "            min_date = df_clean[time_col].min()\n",
    "            max_date = df_clean[time_col].max()\n",
    "            ax.text(0.02, 0.98, f'Date Range: {min_date.strftime(\"%Y-%m-%d\")} to {max_date.strftime(\"%Y-%m-%d\")}',\n",
    "                   transform=ax.transAxes, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸ No timestamp columns found in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee86a439",
   "metadata": {},
   "source": [
    "## 8. Correlation Analysis\n",
    "\n",
    "Analyze correlations between different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6880424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis for tables with multiple numeric columns\n",
    "for table_name, df in sample_data.items():\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(numeric_cols) >= 3:  # Need at least 3 columns for meaningful correlation\n",
    "        print(f\"\\nðŸ”— Correlation Analysis for {table_name}:\")\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = df[numeric_cols].corr()\n",
    "        \n",
    "        # Create correlation heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # Mask upper triangle\n",
    "        \n",
    "        sns.heatmap(corr_matrix, \n",
    "                   mask=mask,\n",
    "                   annot=True, \n",
    "                   cmap='RdYlBu_r', \n",
    "                   center=0,\n",
    "                   square=True,\n",
    "                   linewidths=0.5,\n",
    "                   fmt='.2f',\n",
    "                   cbar_kws={\"shrink\": .8})\n",
    "        \n",
    "        plt.title(f'ðŸ”¥ Correlation Heatmap - {table_name}', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Find strong correlations (> 0.7 or < -0.7)\n",
    "        strong_corr = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                corr_val = corr_matrix.iloc[i, j]\n",
    "                if abs(corr_val) > 0.7:\n",
    "                    strong_corr.append({\n",
    "                        'Variable 1': corr_matrix.columns[i],\n",
    "                        'Variable 2': corr_matrix.columns[j],\n",
    "                        'Correlation': corr_val\n",
    "                    })\n",
    "        \n",
    "        if strong_corr:\n",
    "            strong_corr_df = pd.DataFrame(strong_corr).sort_values('Correlation', key=abs, ascending=False)\n",
    "            print(\"\\nðŸ’ª Strong Correlations (|r| > 0.7):\")\n",
    "            display(strong_corr_df)\n",
    "        else:\n",
    "            print(\"\\nðŸ“ No strong correlations found (|r| > 0.7)\")\n",
    "        \n",
    "        break  # Only show first table to avoid clutter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38000ae1",
   "metadata": {},
   "source": [
    "## 9. Geographic and Regional Analysis\n",
    "\n",
    "Analyze data distribution by geographic regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312fb389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find geographic columns\n",
    "geo_keywords = ['region', 'kabupaten', 'city', 'province', 'area', 'location', 'lat', 'lon', 'coordinate']\n",
    "geo_data = {}\n",
    "\n",
    "for table_name, df in sample_data.items():\n",
    "    geo_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in geo_keywords)]\n",
    "    if geo_cols:\n",
    "        geo_data[table_name] = {'df': df, 'geo_cols': geo_cols}\n",
    "\n",
    "print(f\"ðŸŒ Found geographic data in {len(geo_data)} tables:\")\n",
    "for table, data in geo_data.items():\n",
    "    print(f\"   {table}: {data['geo_cols']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9179d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographic distribution analysis\n",
    "if geo_data:\n",
    "    for table_name, data in geo_data.items():\n",
    "        df = data['df']\n",
    "        geo_cols = data['geo_cols']\n",
    "        \n",
    "        print(f\"\\nðŸ—ºï¸ Geographic Analysis for {table_name}:\")\n",
    "        \n",
    "        # Analyze each geographic column\n",
    "        for col in geo_cols[:2]:  # Limit to first 2 columns\n",
    "            if df[col].dtype == 'object' or df[col].dtype.name == 'category':\n",
    "                # Categorical geographic data\n",
    "                value_counts = df[col].value_counts().head(10)\n",
    "                \n",
    "                if len(value_counts) > 1:\n",
    "                    plt.figure(figsize=(12, 6))\n",
    "                    \n",
    "                    # Bar plot\n",
    "                    bars = plt.bar(range(len(value_counts)), value_counts.values, color=colors[:len(value_counts)])\n",
    "                    plt.title(f'ðŸ“ Distribution by {col} - {table_name}', fontweight='bold')\n",
    "                    plt.xlabel(col.title())\n",
    "                    plt.ylabel('Number of Records')\n",
    "                    plt.xticks(range(len(value_counts)), value_counts.index, rotation=45, ha='right')\n",
    "                    \n",
    "                    # Add value labels\n",
    "                    for bar, value in zip(bars, value_counts.values):\n",
    "                        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01*max(value_counts.values),\n",
    "                                f'{value:,}', ha='center', va='bottom')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    print(f\"\\nðŸ“Š Top 10 {col} values:\")\n",
    "                    for location, count in value_counts.items():\n",
    "                        print(f\"   {location}: {count:,} records\")\n",
    "        \n",
    "        break  # Only show first table\n",
    "else:\n",
    "    print(\"âš ï¸ No geographic columns identified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b6a8a9",
   "metadata": {},
   "source": [
    "## 10. Data Quality Assessment\n",
    "\n",
    "Assess data quality including missing values, duplicates, and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b25200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality analysis\n",
    "quality_report = {}\n",
    "\n",
    "for table_name, df in sample_data.items():\n",
    "    print(f\"\\nðŸ” Data Quality Assessment for {table_name}:\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_rows = len(df)\n",
    "    total_cols = len(df.columns)\n",
    "    \n",
    "    # Missing values\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_percentages = (missing_counts / total_rows) * 100\n",
    "    \n",
    "    # Duplicate rows\n",
    "    duplicate_rows = df.duplicated().sum()\n",
    "    duplicate_percentage = (duplicate_rows / total_rows) * 100\n",
    "    \n",
    "    # Columns with all missing values\n",
    "    empty_cols = missing_counts[missing_counts == total_rows].index.tolist()\n",
    "    \n",
    "    # Numeric columns with potential outliers\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    outlier_cols = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if df[col].notna().sum() > 10:  # At least 10 non-null values\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)][col]\n",
    "            if len(outliers) > 0:\n",
    "                outlier_cols.append({'column': col, 'outlier_count': len(outliers), 'outlier_pct': len(outliers)/len(df)*100})\n",
    "    \n",
    "    # Store quality metrics\n",
    "    quality_report[table_name] = {\n",
    "        'total_rows': total_rows,\n",
    "        'total_cols': total_cols,\n",
    "        'duplicate_rows': duplicate_rows,\n",
    "        'duplicate_pct': duplicate_percentage,\n",
    "        'empty_columns': len(empty_cols),\n",
    "        'columns_with_missing': len(missing_counts[missing_counts > 0]),\n",
    "        'outlier_columns': len(outlier_cols)\n",
    "    }\n",
    "    \n",
    "    print(f\"   ðŸ“ Dimensions: {total_rows:,} rows Ã— {total_cols} columns\")\n",
    "    print(f\"   ðŸ”„ Duplicate rows: {duplicate_rows:,} ({duplicate_percentage:.1f}%)\")\n",
    "    print(f\"   ðŸ•³ï¸ Empty columns: {len(empty_cols)}\")\n",
    "    print(f\"   â“ Columns with missing data: {len(missing_counts[missing_counts > 0])}/{total_cols}\")\n",
    "    print(f\"   ðŸŽ¯ Columns with outliers: {len(outlier_cols)}\")\n",
    "    \n",
    "    # Show missing data details\n",
    "    high_missing = missing_percentages[missing_percentages > 10].sort_values(ascending=False)\n",
    "    if not high_missing.empty:\n",
    "        print(f\"\\n   âš ï¸ Columns with >10% missing data:\")\n",
    "        for col, pct in high_missing.head(5).items():\n",
    "            print(f\"     â€¢ {col}: {pct:.1f}% missing\")\n",
    "\n",
    "# Create quality summary visualization\n",
    "if quality_report:\n",
    "    quality_df = pd.DataFrame(quality_report).T\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Duplicate percentages\n",
    "    axes[0,0].bar(quality_df.index, quality_df['duplicate_pct'], color='lightcoral')\n",
    "    axes[0,0].set_title('ðŸ”„ Duplicate Rows (%)', fontweight='bold')\n",
    "    axes[0,0].set_ylabel('Percentage')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Columns with missing data\n",
    "    axes[0,1].bar(quality_df.index, quality_df['columns_with_missing'], color='lightblue')\n",
    "    axes[0,1].set_title('â“ Columns with Missing Data', fontweight='bold')\n",
    "    axes[0,1].set_ylabel('Count')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Empty columns\n",
    "    axes[1,0].bar(quality_df.index, quality_df['empty_columns'], color='lightyellow')\n",
    "    axes[1,0].set_title('ðŸ•³ï¸ Completely Empty Columns', fontweight='bold')\n",
    "    axes[1,0].set_ylabel('Count')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Outlier columns\n",
    "    axes[1,1].bar(quality_df.index, quality_df['outlier_columns'], color='lightgreen')\n",
    "    axes[1,1].set_title('ðŸŽ¯ Columns with Outliers', fontweight='bold')\n",
    "    axes[1,1].set_ylabel('Count')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.suptitle('ðŸ“Š Data Quality Assessment Summary', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7f00f8",
   "metadata": {},
   "source": [
    "## 11. Interactive Plotly Visualizations\n",
    "\n",
    "Create interactive charts for deeper exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f45a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive visualizations with Plotly\n",
    "if sample_data:\n",
    "    # Select main table for interactive analysis\n",
    "    main_table = list(sample_data.keys())[0]\n",
    "    main_df = sample_data[main_table]\n",
    "    \n",
    "    # Get numeric columns\n",
    "    numeric_cols = main_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if len(numeric_cols) >= 2:\n",
    "        print(f\"ðŸŽ® Creating interactive visualizations for {main_table}...\")\n",
    "        \n",
    "        # Interactive scatter plot\n",
    "        if len(numeric_cols) >= 2:\n",
    "            col1, col2 = numeric_cols[0], numeric_cols[1]\n",
    "            \n",
    "            # Sample data for performance\n",
    "            plot_df = main_df[[col1, col2]].dropna().sample(min(1000, len(main_df)))\n",
    "            \n",
    "            fig = px.scatter(plot_df, x=col1, y=col2,\n",
    "                           title=f'ðŸ” Interactive Scatter Plot: {col1} vs {col2}',\n",
    "                           labels={col1: col1.title(), col2: col2.title()},\n",
    "                           template='plotly_white')\n",
    "            \n",
    "            fig.update_traces(marker=dict(size=6, opacity=0.6))\n",
    "            fig.update_layout(height=500)\n",
    "            fig.show()\n",
    "        \n",
    "        # Interactive histogram\n",
    "        if len(numeric_cols) >= 1:\n",
    "            col = numeric_cols[0]\n",
    "            data = main_df[col].dropna()\n",
    "            \n",
    "            # Remove extreme outliers for better visualization\n",
    "            Q1 = data.quantile(0.05)\n",
    "            Q3 = data.quantile(0.95)\n",
    "            filtered_data = data[(data >= Q1) & (data <= Q3)]\n",
    "            \n",
    "            fig = px.histogram(x=filtered_data, nbins=50,\n",
    "                             title=f'ðŸ“Š Interactive Histogram: {col}',\n",
    "                             labels={'x': col.title(), 'y': 'Frequency'},\n",
    "                             template='plotly_white')\n",
    "            \n",
    "            fig.update_layout(height=400)\n",
    "            fig.show()\n",
    "    \n",
    "    else:\n",
    "        print(\"âš ï¸ Not enough numeric columns for interactive plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379ca714",
   "metadata": {},
   "source": [
    "## 12. Export and Summary Report\n",
    "\n",
    "Generate a comprehensive summary and save visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de6103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary report\n",
    "print(\"ðŸ“‹ COMPREHENSIVE RAN DATABASE ANALYSIS REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Database overview\n",
    "total_tables = len(tables)\n",
    "total_sample_rows = sum(len(df) for df in sample_data.values())\n",
    "total_columns = sum(len(df.columns) for df in sample_data.values())\n",
    "\n",
    "print(f\"\\nðŸ—ï¸ DATABASE OVERVIEW:\")\n",
    "print(f\"   â€¢ Total Tables: {total_tables}\")\n",
    "print(f\"   â€¢ Sample Rows Analyzed: {total_sample_rows:,}\")\n",
    "print(f\"   â€¢ Total Columns: {total_columns}\")\n",
    "print(f\"   â€¢ Database Size: {DATABASE_PATH.stat().st_size / (1024*1024):.1f} MB\")\n",
    "\n",
    "# Technology distribution\n",
    "print(f\"\\nðŸ“¡ TECHNOLOGY DISTRIBUTION:\")\n",
    "for tech, count in tech_distribution.items():\n",
    "    print(f\"   â€¢ {tech}: {count:,} records ({count/sum(tech_distribution.values())*100:.1f}%)\")\n",
    "\n",
    "# Data quality summary\n",
    "if quality_report:\n",
    "    print(f\"\\nðŸ” DATA QUALITY SUMMARY:\")\n",
    "    avg_duplicates = np.mean([report['duplicate_pct'] for report in quality_report.values()])\n",
    "    total_empty_cols = sum([report['empty_columns'] for report in quality_report.values()])\n",
    "    total_outlier_cols = sum([report['outlier_columns'] for report in quality_report.values()])\n",
    "    \n",
    "    print(f\"   â€¢ Average Duplicate Rate: {avg_duplicates:.1f}%\")\n",
    "    print(f\"   â€¢ Total Empty Columns: {total_empty_cols}\")\n",
    "    print(f\"   â€¢ Columns with Outliers: {total_outlier_cols}\")\n",
    "\n",
    "# Performance metrics found\n",
    "if perf_columns:\n",
    "    total_perf_metrics = sum(len(cols) for cols in perf_columns.values())\n",
    "    print(f\"\\nðŸŽ¯ PERFORMANCE METRICS:\")\n",
    "    print(f\"   â€¢ Tables with Performance Data: {len(perf_columns)}\")\n",
    "    print(f\"   â€¢ Total Performance Metrics: {total_perf_metrics}\")\n",
    "\n",
    "# Geographic coverage\n",
    "if geo_data:\n",
    "    print(f\"\\nðŸŒ GEOGRAPHIC COVERAGE:\")\n",
    "    print(f\"   â€¢ Tables with Geographic Data: {len(geo_data)}\")\n",
    "    for table, data in geo_data.items():\n",
    "        print(f\"   â€¢ {table}: {len(data['geo_cols'])} geographic columns\")\n",
    "\n",
    "# Time coverage\n",
    "if time_data:\n",
    "    print(f\"\\nâ° TIME COVERAGE:\")\n",
    "    for table, data in time_data.items():\n",
    "        df = data['df']\n",
    "        time_col = data['time_col']\n",
    "        df_clean = df.dropna(subset=[time_col])\n",
    "        if len(df_clean) > 0:\n",
    "            min_date = df_clean[time_col].min()\n",
    "            max_date = df_clean[time_col].max()\n",
    "            print(f\"   â€¢ {table}: {min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(f\"\\nâœ… ANALYSIS COMPLETE!\")\n",
    "print(f\"ðŸ“Š Generated visualizations include:\")\n",
    "print(f\"   â€¢ Technology distribution charts\")\n",
    "print(f\"   â€¢ Performance metrics histograms\")\n",
    "print(f\"   â€¢ Time series analysis\")\n",
    "print(f\"   â€¢ Correlation heatmaps\")\n",
    "print(f\"   â€¢ Geographic distribution plots\")\n",
    "print(f\"   â€¢ Data quality assessments\")\n",
    "print(f\"   â€¢ Interactive Plotly visualizations\")\n",
    "\n",
    "print(f\"\\nðŸš€ NEXT STEPS:\")\n",
    "print(f\"   1. Use insights for NER model training\")\n",
    "print(f\"   2. Develop SQL query templates based on schema\")\n",
    "print(f\"   3. Create performance benchmarks\")\n",
    "print(f\"   4. Build automated monitoring dashboards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e685942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analysis results (optional)\n",
    "output_dir = PROJECT_ROOT / \"data\" / \"processed\" / \"analysis_results\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save table summary\n",
    "if 'summary_df' in locals():\n",
    "    summary_df.to_csv(output_dir / \"table_summary.csv\", index=False)\n",
    "    print(f\"ðŸ’¾ Table summary saved to: {output_dir / 'table_summary.csv'}\")\n",
    "\n",
    "# Save quality report\n",
    "if quality_report:\n",
    "    quality_df = pd.DataFrame(quality_report).T\n",
    "    quality_df.to_csv(output_dir / \"data_quality_report.csv\")\n",
    "    print(f\"ðŸ’¾ Quality report saved to: {output_dir / 'data_quality_report.csv'}\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Analysis complete! All results available in the notebook and saved files.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
